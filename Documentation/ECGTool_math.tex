\chapter{Mathematical Background} \label{sec:math}

\section{Overview}

In this chapter we describe the basic mathematical formulations and some
solution approaches to the forward and inverse problems of
electrocardiography. The goal of solutions to the forward problem, also called ECG
forward Simulation, is to
predict potentials that could be measured in any accessible location
(usually the surface of the torso) given a description of the cardiac
electrical sources as well as the geometry and conductivities of the torso involved.
The goal of solutions to the inverse problem, also called ECG imaging or ECGI,
is to predict cardiac sources given a set
of measurements and the same geometry and conductivity information. It is
important to note that a solution to ECG Imaging presupposes an
available solution to the forward simulation.
Here we briefly summarize the basic background to
facilitate our description of the toolkit capabilities in subsequent chapters.
Please refer to the list of literature on the CEI webpage
for more information (\url{http://www.ecg-imaging.org/home/publications}).


Solving both forward and inverse problems requires a specific model formulation of
the cardiac electrical sources. This toolkit contains examples from the two arguably
most dominant equivalent source models used in current forward and inverse
problem research. One model, which we will refer to as the ``activation-based''
source model, assumes that the dominant feature of cardiac electrical activity
is the timing of the arrival of the depolarization wavefront (known as ``activation times'')
at each location in the heart. (A similar problem, in which the equivalent
sources called ''recovery times'' are the timing of repolarization at each
location, is solved in a similar fashion.) Classical results have shown that,
under assumptions of isotropy and homogeneity of
the myocardium, activation-based models can be reduced to the activation
times on the surface of the heart \cite{RSM:Oos2004}. In this context, this is usually taken as
the epicardial (outer) and endocardial (inner) heart surfaces, connected
across the base of the heart by an imaginary connecting surface. The source
in this case can be modeled by a moving set of current dipoles aligned
along the ``activation wavefront''; that is, the curve where activation is
taking place on this surface at any given time.

The second source model treated here, which we will refer to as the
``potential-based'' source model, assumes that the cardiac sources can be
represented by the time-varying electrical potentials present on a surface,
enclosing all the electrical sources. Gauss' Law implies that any such set
of potentials is unique, and that the closer the surface is to the myocardial
surface the more useful the model is. So, the surface is typically taken as
the epicardium, closed off by an imaginary ``top'' surface at the base of
the heart or, alternatively, the same joint epicardial/endocardial surface
used for activation-based models.

In the rest of this chapter we describe solutions to the forward and
inverse problem concentrating on the specific tools currently provided in
this toolkit. Again we refer the reader to the literature for more complete
background.

%%:
%%:
%%: The
%%: relationship of the torso potentials and heart source in generally
%%: expressed as $y=A(x)$ with $y$ as the torso potential vector, $x$ as the
%%: heart sources, and $A$ as the function that relates the torso surface to
%%: the heart surface. In this chapter, we will provide examples of different
%%: ways to formulate and compute the function $A$ and the torso potentials
%%: from heart sources using SCIRun.
%%:
%%: There are mainly two types of heart sources used in the forward problem:
%%: cardiac surface potentials, and activation times. The potential based
%%: forward problem is more intuitive and can be thought of as a projection of
%%: the cardiac potentials through the torso volume to the surface. Each point
%%: on the heart surface contributes directly to the torso surface potential in
%%: a way described by the transfer function $A$.  In this case of the
%%: potential-based forward problem, $A$ is a matrix, and the forward problem
%%: is solve by a matrix multiplication: $y=A \cdot x$, $x$ being the cardiac
%%: potential vector. The activation-time base-forward model is based on the
%%: concept that activation wavefront of the myocardium contains a high
%%: potential gradient with the rest of the heart either activated or resting,
%%: generating a current source along the wavefront. Using a uniform dipole
%%: layer to represent the source of the wavefront, one can calculate the torso
%%: surface potentials.
%%:
\section{Solutions to the Forward Problem in the Forward/Inverse Toolkit}

The temporal frequencies which are relevant to electrocardiographic
bioelectricity are relatively low, and the wavelengths many orders of
magnitude larger than the dimensions of the human body. So, from a
bioelectricity viewpoint, the governing partial differential equation (PDE)
is Laplace's equation:

\begin{equation} \nabla \cdot (\BM{\sigma} \nabla \Phi) = 0, \label{eq:eq}
\end{equation}
\noindent where $\BM{\sigma}$ contain the relevant conductivities and $\Phi$ the
electrical potentials. The boundary conditions are given by:
\begin{align} \Phi(x,y,z)|_{\Omega_k} &= V_k\\ \left. \frac{\partial
\Phi}{\partial \hat{n}} \right|_\Omega &= 0 \label{eq:bc}
\end{align}
\noindent where $\Omega_{k}$ is the surface on which the sources are located,
$V_k$ are the potentials on that surface, and $\hat{n}$ are the surface normals
of the surface.

As an alternative to solving this problem directly, a (weighted) integral
can be taken over the solution domain on both sides of \autoref{eq:eq}
and the resulting integral equation solved for $\Phi$ at locations of
interest. This is usually referred to as the ``weak form'' of the PDE
solution, and (aside from discontinuities which theoretically could be
possible in the weak form solution but are not of practical importance
here) is equivalent to solving the original, or ``strong'' form.

In a tractable geometry, such as a set of concentric or even eccentric
spheres, this PDE can be solved via analytical expansions. However in
complex geometries such as realistic torso models, numerical solutions must
be applied.
%Again there is a large literature on such numerical methods.
Two of these methods have predominated in the literature for
forward electrocardiography: the Finite Element Method (FEM) and the
Boundary Element Method (BEM). It is these two methods which have been
implemented in this toolkit. Thus, in the rest of this subsection, we give a
very brief description of these two methods.

The major difference between FEM and BEM, from the practical application point of view,
is in the way in which they discretize the solution domain.
FEM relies on a volume discretization. The geometry of the solution domain
is describe by a mesh of small three dimensional volume elements,
each with its own conductivity parameters.
BEM, on the other hand, is a surface discretization
method, with the geometry represented as a collection of bounding surfaces
separating regions in the volume with different conductivities. Each of
these surfaces is then discretized into a mesh. In other words, in both FEM
and BEM there exists a collection of points, called nodes, which define the
respective volume or surface elements. The potential $\Phi$ (and the
current, $\BM{\sigma} \nabla \Phi)$, is approximated by interpolating the
potential and current across those elements based on its value at the
nodes using known (usually polynomial) interpolation functions. Thus,
numerical integration can be applied to the weak form, and the node values
come out of the integrals, leaving subintegrals over known functions which
result in a set of weights. The result in either case is a system of
linear equations.

The boundary conditions in \autoref{eq:bc} are applied differently  in FEM
and BEM, another important difference between the two methods.


%%:
%%: %%: collection of at surface elements. However, the BEM method is a
%%: global method in that all points ``see'' each other, resulting in a
%%: dense-matrix formulation while the basis functions used in FEM have
%%: compact support, resulting in a sparse-matrix formulation.
%%:

We note that either BEM or FEM can be applied to both activation-based and
potential-based source models, although there are important implementation
details. We briefly note these differences here, and some specifics of the
applications in the context of the relevant SCIRun modules will be
presented in Ch.~\ref{ch:fwd}.


%%: The inverse problem of electrocardiography is to invert the model in the forward problem or, in other words, to solve for sources on the heart that suitably fit the given body surface potentials and forward model. This forward relationship can be described by the equation $y=A(x)$ where $y$ is the vector of body surface potentials, $A$ is the forward function, and $x$ are the sources on the heart.
%%:
%%: Inverse solution methods can be split into two categories based on their source models: activation-based and potential-based. Potential-based source formulations typically assume that the sources are parameterized by electric potentials themselves and the forward function is linear matrix multiplication: $y=Ax$. Activation-based source formulations...
%%:

\subsection{FEM in the Forward/Inverse Toolkit}


%%: The Finite Element Method is a volume-discretization method and is
%%therefore a good choice when either the solution is desired throughout
%%the entire volume or when there are detailed volume-varying parameters,
%%such as conductivity, that are to be accounted for. FEM is fairly complex
%%and volumes of literature exist on the method. Below is a brief overview
%%of some of the key points of the method.
%%:

%%: As was previously stated, the steady state electrical potential in an
%%: inhomogeneous volume conductor is described by the equation
%%: %
%%: \begin{equation} \nabla \cdot (\BM{\sigma} \nabla \Phi) =
%%: 0, \label{eq:eq}.
%%: \end{equation}
%%: %
%%: and again, the boundary conditions are given by:
%%: \begin{align} \Phi(x,y,z)|_{\Omega_k} &= V_k\\ \left. \frac{\partial
%%: \Phi}{\partial n} \right|_\Omega &= 0.
%%: \end{align}
%%:
%%:

The finite element method begins by subdividing the geometry into a set of
volume elements with vertices at a set of nodes, and then approximating
the potential in the volume by a basis expansion:
\begin{equation} \bar{\Phi}(x,y,z) = \sum_i \Phi_i
N_i(x,y,z), \label{eq:approx}
\end{equation}
\noindent where $\{N_i\}$ are a set of basis functions, one for each node in the
volume element discretization, and
$\{\Phi_i\}$ are the corresponding (unknown) coefficients at those nodes.
Note that if the
$\{\Phi_i\}$ can be determined, then the potential everywhere in the volume
can be approximated via the basis expansion in
\autoref{eq:approx}. Usually, the basis functions are (Cartesian product)
low-order polynomials, most commonly tri-linear functions, designed so that
each function is $1$ at its ``own'' node and decays to $0$ at the other nodes of
all elements which share that node (in which case $\Phi_{i}$ becomes a
direct approximation of the potential at node $i$).


The Galerkin method is applied to solve this Laplace equation. In
particular, the basis expansion in \autoref{eq:approx} is substituted
into \autoref{eq:eq}, both sides of the equation are multiplied by a set
of ``trial'' or ``test'' functions (typically taken to be the same family of
functions as the basis functions $\{N_{i}\}$), and then the equation is
integrated over the solution domain, resulting in a weak form of the
PDE.
%%: %
%%: This is the so called ``strong form'' of the equation. The weak form
%%: comes by integrating both sides of (\ref{eq:strong}) against a ``trial
%%: function'' (in this case we will use $N_j$ as a trial function, chosen
%%: from the same set of basis functions as above. This leaves us with
%%: \begin{equation} \int_\Omega \nabla \cdot (\BM{\sigma} \nabla \sum_i
%%: \Phi_i N_i)N_j \, d\mathbf{V} = \int_\Omega 0 \cdot N_j \,
%%: d\mathbf{V}.
%%: \end{equation}
%%: %
%%:

Manipulation of the resulting integral equations yields, in the case where
the test and basis function sets are the same,

\begin{equation} \sum_i \Phi_i \int_{\Omega - \bar{\Omega} -
\bar{\Omega}_k} \BM{\sigma} \nabla N_i \nabla N_j \, d\mathbf{V} = 0. \label{eq:int_eq}
\end{equation}

This can be rewritten as the matrix vector equation:
\begin{equation} \mathbf{K} \BM{\Phi} = 0 \label{eq:mat_eq}
\end{equation}
%
\noindent where $\mathbf{K}_{ij} = \int \BM{\sigma} \nabla N_i \nabla N_j \,
d\mathbf{V}$ is the stiffness matrix, and $\BM{\Phi} = [\Phi_1,
\ldots, \Phi_n]^T$ is the vector of unknown coefficients. The critical
point is that the coefficients in $\mathbf{K}$ depend only on the geometry
and the choice of basis and test functions, and thus can be computed ahead
of time. We note that this equation is clearly singular, and an additional
condition must be imposed (biophysically equivalent to taking some
potential value as a reference) to reduce the number of degrees of freedom
by one.

Once the equations are written, the boundary conditions must be imposed. In
the case of bioelectric field problems such as forward electrocardiography,
this reduces to replacing the $0$'s on the right hand side by known
currents or fixing some values of the vector $\BM{\Phi}$ to correspond to
known voltages. There are a variety of ways to accomplish this to preserve
certain numerical properties of the matrix equation. If the
measurement electrodes are treated as being larger than one node in size,
this can lead to additional boundary conditions which in turn leads to
additional modifications of the equations.

The result is a matrix equation whose size is the total number of nodes in
the volume, which tends to result in a relatively large system. However, as
long as the basis and test functions have local support (for example, with
linear basis functions, the support is restricted to all first-order
neighboring nodes of the given node), most of the integrals defining the
elements of $\mathbf{K}$ will involve non-overlapping functions and thus be
equal to $0$. Therefore, $\mathbf{K}$ will be a very sparse matrix with
strong structure, leading to the possibility of both efficient storage and
efficient solution by iterative solvers.

We note that every time a different set of source currents
(activation-based model) or potentials (potential-based model) is applied
on the heart surface, the modifications of the stiffness matrix are
different, and thus the system of equations must be solved \textit{de
novo}.
However, there exist certain problems where only the values of the potential at
a subset of nodes is needed; In our applications of forward electrocardiography,
we are generally only interested in the solution on the measurement
surface, \textit{e.g.}, on the body surface. Additionally, these applications often
require multiple solutions performed for the given geometry, \textit{e.g.},
when a time series of body surface potentials needs to be generated
from a time series of sources.
In this case, it can be useful to extract a ``transfer matrix''
from \autoref{eq:mat_eq}, which directly relates the known sources to the
unknown and desired measurement potentials. Once this is done, solving the
forward problem reduces to matrix-vector multiplication rather than the
solution of a linear system.

There are several approaches to this problem. In this toolkit we have
provided an example SCIRun network to implement one of them, the so-called
``lead field'' method, which solves the matrix equation repeatedly for
source vectors consisting of a $1$ at each source node in turn and $0$ at
all other source nodes \cite{JDT:Gul97}. From this collection of solutions we
can obtain the desired transfer matrix, which we will denote as $\mathbf{A}$
as used in \autoref{eq:TransMat}.
This network is described below in Ch.~\ref{ch:fwd}.

\subsection{BEM in the Forward/Inverse Toolkit}

Expanding on the initial description above, the boundary element method starts with the assumption
that the domain can be divided into a (relatively) small number of
(relatively) large subdomains in which the conductivities are isotropic
(scalar) and constant. In addition there are other conditions on the
subdomains, principally that they be bounded by closed surfaces.
They can be simply nested or can have a more complex arrangement. Given
that assumption, the surfaces of those subdomains become a sufficient
domain upon which to solve the problem for the entire domain.


Briefly, one of the Green's Theorems from vector calculus is applied to an
integrated form of Laplace's equation to transform the differential problem
into a Fredholm integral problem \cite{RSM:Bar77}. The surfaces are each
subdivided (tessellated) into a collection of small surface (or boundary)
elements. Then (two-dimensional) basis functions (again usually low-order
polynomials) are used to approximate the quantities of interest between the
nodes of the resulting surface meshes. Given this discretization, after
manipulation of the resulting integral equation, the integrals required can
be computed through a series of numerical integrations over the mesh
elements. In the BEM method, these integrals involve as unknowns the
potential and its gradient. The integration involves the computation of the
distance between each node within the surface and to all others surfaces.
In complicated geometries, and in all cases when the node is integrated
against the points on its ``own'' surface, there are numerical difficulties
computing these integrals. In those cases there are a number of
sophisticated solutions which have been proposed in the literature (and
some of them are adopted in the SCIRun implementation). The result of all
these integrals is a transfer matrix, which again we will denote
$\mathbf{A}$, relating the source potentials or currents to the unknown
measurement potentials. In the BEM case, because of the all-to-all nature
of the integrations required, this matrix will be dense, not sparse. On the
other hand, the size of the equation will be directly determined by the
number of measurements and sources rather than the number of nodes in the
entire domain. (We note that there is an alternative formulation of the BEM
method which retains the potentials at all nodes on all surfaces, and which
can be reduced to the transfer matrix described here, but we omit the
details as usual.)


\section{Solutions to the Inverse Problem in the Forward/Inverse Toolkit}

To describe the solution to the inverse problem in a manner useful for this
toolkit, we start with two different equations, depending on whether the
activation-based or potential-based source models were used. Both
approaches assume the availability of a forward transfer matrix
$\mathbf{A}$, calculated by any appropriate method, including either FEM or
BEM.

In the activation-based case, the source model is that the unknowns
are an activation surface. (That is, activation times as a function of
position on the heart surface, which we denote as $\tau(x)$, where $x$
indicates position on the heart surface.) The assumptions required for
the activation-based model imply that the temporal waveform of the
potential (and current) at each source node has a fixed form, the same
at all locations on the surface. This is assumed to be either a step
function or a smoothed version of a step function (using piecewise
polynomials or inverse trigonometric functions). We denote this
function as $u(t)$. Thus the relevant forward equation can be written
as

\begin{equation} y(p,t) = \int_{x} \mathbf{A}_{p,x}u(t-\tau(x))\,dx \label{eq:act}
\end{equation}
%
\noindent where the integral is over the heart surface, $\mathbf{A}_{p,x}$ is the
element of $\mathbf{A}$ relating source node $x$ to measurement node $p$,
and $y(p,t)$ is the measurement surface potential at any time $t$ and at a
position $p$ on the body surface. One advantage of the activation-based
formulation is that the number of unknowns over an entire cardiac cycle is
the number of solution nodes taken on the heart surface. On the other hand,
as can be seen in \autoref{eq:act}, the forward equation is non-linear in
the unknown activation times.

In the potential-based case, the forward matrix can be applied in a more
straightforward manner. If we collect all measurements at a given time $t$ into
a vector $\mathbf{y(t)}$ and the potential at all desired heart surface locations
into a vector $\mathbf{x(t)}$, then we have

\begin{equation} \mathbf{y}(t) = \mathbf{A}\mathbf{x}(t).\label{eq:TransMat}
\end{equation}

The resulting equations over a time series can be collected into a
matrix-matrix equation (with columns indexing time samples) or a single
block matrix equation with a block diagonal matrix which has $\mathbf{A}$
repeated along the diagonals. The number of unknowns for the potential-based
inverse problem is then the product of the number of surface nodes and the
number of time samples. The time waveforms are left unconstrained, but
the equations remain linear in the unknowns.

Both approaches result in ill-conditioned systems of equations, a direct
result of the fact that the inverse problem itself is intrinsically
ill-posed. Thus effective numerical solutions need to impose additional
\textit{a priori} constraints to achieve useful solutions, usually through
a technique known as ``regularization.'' Much of the research on the
inverse problem over the last 30+ years has concerned methods of
regularizing this problem.

\subsection{Activation-based Inverse Solutions in the Forward/Inverse Toolkit}

Since the acitvation-based inverse problem is non-linear, iterative solutions
are employed. An ``initial guess'', or starting point, is required. Then
solutions are iteratively re-computed until a desired convergence criterion
is met. Currently in the toolkit, we have a Matlab version of a Gauss-Newton
iterative solver which can be called from within SCIRun. The starting
solution must be supplied by the user. (We refer the reader to
Ch.~\ref{ch:inv} for details.) Regularization is done using a constraint on
the $\ell_{2}$ norm of the Laplacian of the solution. See the next subsection
for an explanation of Tihkhonov regularization.

\subsection{Potential-based Inverse Solutions in the Forward/Inverse Toolkit}

To combat the ill-posedness of the (linear) potential-based inverse
problem, some form of what is known as ``regularization'' is typically
applied. A number of such solution methods are available through the
toolkit, either native in SCIRun or through the Matlab interface. Here we
describe the basic formulation behind these approaches, while usage details
are in Ch.~\ref{ch:inv}.

\subsubsection{Standard Tikhonov regularization}
\label{sec:math_tikhonov}

The Tikhonov regularization minimizes $ \| Ax - y \|$ in order to solve $Ax=y$,
which is the same as \autoref{eq:TransMat} with $x$ replacing $x(t)$ for the
unknown solution and $y$ replacing $y(t)$ for the given measurement. The
forward solution matrix $\mathbf{A}$ is of size $m\times n$, where $m$ is the
number of measurement channels and $n$ is the number of source reconstruction
points). The system $Ax$ can be overdetermined ($m > n$), underdetermined
($m <n$) or $m=n$. $A$ is often ill-conditioned or singular, so it needs to be
regularized. The Tikhonov regularization is often used to overcome those
problems by introducing a minimum solution norm constraint, such as
$\lambda \|Lx\|_2^2$. The solution of the problem can be found by
minimizing the following function:
\begin{center}
\begin{eqnarray}
    f (x) = \| P (y - A x) \|^{2}_{2} + \lambda^{2} \| Lx \|^{2}_{2},
\label{tik_problem}
\end{eqnarray}
\end{center}


\noindent where $\lambda$ is the regularization parameter, which is a user
defined scalar value. The matrix $\mathbf{P}$ represents the \textit{a
priori} knowledge of the measurements. The matrix $\mathbf{L}$ describes
the property of the solution $x$ to be constrained. Conceptually, $\lambda$
trades off between the misfit between predicted and measured data (the
first term in the equation) and the \textit{a priori} constraint. An
approximate solution $\hat{x}$ of \autoref{tik_problem} is given for the
overdetermined case ($n > m$) as follows:
\begin{center}
\begin{eqnarray}
    \| P^{-1} (y - A x) \|^{2}_{2} + \lambda^{2} \| L x \|^{2}_{2} &=& f(x)
\nonumber \\
    (A^{T} P^{T} P A + \lambda^{2} (L^{T} L)^{-1}) \hat{x} &=& A^{T} P^{T} P
y \\
\label{tik_problem_overdet}
\end{eqnarray}
\end{center}

\noindent and for the underdetermined case ($n < m$) as:

\begin{center}
\begin{eqnarray}
   \| C^{-1} (y - A x) \|^{2}_{2} + \lambda^{2} \| Wx \|^{2}_{2} &=& f(x)
\nonumber\\
   W {W}^{T} A^{T} (A W {W}^{T} A^{T} + \lambda^{2} (C
C^{T})^{^-1})^{-1} y &=& \hat{x} \\
\label{tik_problem_underdet}
\end{eqnarray}
\end{center}
\noindent with source space weighting $ W,L \in n \times n $ and sensor space
weighting $ C,P \in m \times m $.

When $n=m$, either of the above formulations can be used.
%Inverting the weighting matrices depends on the case and used the reconstruction method (so the posed apriori knowledge).
Both solutions are equal but the computational effort may differ. Furthermore, both representations can be solved numerically by a direct method (\textit{e.g.} Gaussian elimination) or an iterative method (\textit{e.g.} conjugate gradients). Both methods are implemented in SCIRun.

An alternative approach to solve the Tikhonov minimization involves a singular value
decomposition (SVD) of the matrix $\mathbf{A}$. The SVD results in several factor matrices, from which one then obtains the inverse solution. The SVD approach is also implemented in SCIRun and will be described below.

\subsubsection{Choosing the regularization parameter:}
\label{sec:math_regparam}

Choosing an appropriate value as a regularization parameter is critical for achieving useful solutions.
The solution typically depends on it in a sensitive fashion, and good
choices vary with the size, smoothness, etc. of the problem/solution as well as on the
choice of regularization constraint. One commonly-used method to choose the
regularization parameter automatically is to run the inverse solution for a
large variety of parameters. For all regularization parameters, the residual
norm (fit of the data, $\| y - A \hat{x} \|^{2}_{2} $) and the weighted
solution norm (solution properties, $\|L\hat{x} \|^{2}_{2}$) is plotted in a log-log plot. Often,
the resulting curve shape looks similar to the letter
"L". The corner of the "L" represents a good trade off between both
constraints. The automatic parameter selection used in this L-curve approach
can be used for underdetermined and the overdetermined case as in option in
the module {\tt SolveInverseProblemWithTikhonov}.

\subsubsection{Truncated SVD Tikhonov regularization}

Another approach to regularization is to approximate $\mathbf{A}$ with a
low-rank substitute by truncating the low-order modes of its SVD. The
choice of rank is equivalent to the choice of regularization parameter.

\subsubsection{Isotropy Method}

This inverse method was introduced in the literature by Huiskamp and
Greensite.
It attempts to decorrelate the time series of the source waveforms prior to
applying spatial regularization.
However, since the temporal correlation of the source potentials
is unknown, the isotropy method approximates this by the temporal
correlation of the measurements. Under certain conditions (termed ``isotropy'' by the
authors of this method, but also known as ``separability'' in
the random field literature)
this produces an equivalent decorrelating basis. Once this decorrelation is
achieved, the resulting set of equations is truncated and then standard
Tikhonov regularization is applied to each remaining system. After solving
these systems, the decorrelation is reversed to restore the
temporal correlation which had been estimated and removed previously.

\subsubsection{Spline-Based Inverse Method}

As in the Isotropy method, the Spline-Based method enforces prior knowledge in time.
This method, introduced by Erem {\it et.al.}, enforces a smooth temporal behavior of the potentials on the heart through the paramaterization with a spline curve.
The main assumption behind this model is that the sequence of multi-dimensional potentials, discretetized on the body surface by electrodes and heart by nodes on the geometry, across time lie on a 1D manifold embedded in a high dimensional space.
Thus, position along this curve is equivalent to phase of cardiac cycle.
As in the Isotropy method, the spline-inverse first attempts to estimate the temporal structure of the heart potentials by fitting a smooth-varying spline on the measured torso potentials.
Then, it solves the inverse problem for the reduced set of knot points that characterize the fitted spline with a first order Tikhonov inverse method.
The full temporal sequence of heart potentials is finally reconstructed by combining the inverse solutions obtained for each knot point with the temporal structure obtained from the torso potentials.

\subsubsection{Total Variation Method} \label{sec:inv-tv}

The effect of the $\ell_{2}$-norm regularization in \autoref{tik_problem} is
typically to produce smooth solutions, where the smoothness is taken as a
worthwhile cost to increase reliability in the face of
ill-conditioning. However, cardiac wavefronts are relatively sharp
(non-smooth) in space, and thus Tikhonov methods typically produce overly
smooth wavefronts. One approach to address this problem is to replace the
typical regularization constraints with a ``total variation'' (TV) constraint,
which is to constrain the $\ell_{1}$ norm of the gradient of the
solution. Minimization of $\ell_{1}$ norms favors a small number of
relatively large values in the solution compared to $\ell_{2}$ norm
minimization, so using TV constraints will favor a sparse set of
rapid spatial changes, which may allow reconstruction of wavefronts more
accurately.

Specifically, total variation regularization can be formulated as follows:
%
\begin{eqnarray}
     \mathbf{\hat{x}} &=& \mbox{argmin   } \left\|\mathbf{A}\mathbf{x} -
     \mathbf{y}\right\|^2_{2} + \lambda TV(\mathbf{x}) \label{eq:tv} \\
     TV(\mathbf{u}) &=&  \left\| \mathbf{L}\mathbf{u} \right\|_{} = \sum_i |\mathbf{L}\mathbf{u}|_i \label{eq:tv-term}
\end{eqnarray}
%
\noindent where $\mathbf{L}$ is a matrix approximation of the spatial gradient and
the last sum is over the elements of the matrix-vector product.


To solve this minimization problem, \autoref{eq:tv} is differentiated
with respect to $\mathbf{x}$ and the gradient is set to zero. Because the total variation term, $TV(u)$, is not differentiable at zero, a positive constant $\beta$ is added:
%
\begin{eqnarray} \label{eqn:tv-term2}
TV(\mathbf{u}) = \sum_i \sqrt{(|\mathbf{L}\mathbf{u}|_i)^2 + \beta^2}
\end{eqnarray}

This results in the need to solve
%
\begin{equation}\label{eq:tv3}
(\mathbf{A}^T\mathbf{A}+\lambda
\mathbf{L}^T\mathbf{W}_{\beta}(\mathbf{x})\mathbf{L}) \mathbf{x} =
\mathbf{A}^T\mathbf{y}
\end{equation}
%
\noindent where the diagonal weight matrix $\mathbf{W}_{\beta}$ is defined as:
%
\begin{equation} \label{eq:tv-weightmat}
\mathbf{W}_{\beta}(u) = \frac{1}{2} \mbox{diag} \left[ 1 / \sqrt{|[\mathbf{L}\mathbf{x}]_i|^2 + \beta^2}\right]
\end{equation}

It can be seen that the weight matrix $\mathbf{W}$ depends on the local
derivative. When the local derivative is small, the weight becomes a large
value, imposing greater smoothness on the solution. When the local derive
is large, the weight becomes a small value, making the solution less
constrained.

The total variation method is non-linear because the weight matrix
$\mathbf{W}$ relies on the solution. We solve this iteratively, with an all-zero initial
guess.

The total variation regularization contains two parameters to be tuned:
$\beta$ and $\lambda$. $\beta$ controls the smoothness of the computed
solution. A small $\beta$ allows sharp gradients in the solution. $\lambda$
controls the amount of regularization.

\subsubsection{Wavefront-based potential reconstruction (WBPR) method}

The WBPR method, like TV, also attempts to impose a constraint which
encourages the presence of wavefronts in the solution. Simply put, the idea
is to locate the wavefront by an appropriate ``jump-detection'' algorithm at each time instant.
Then the solution is approximated with one that has three regions: a non-activated region (which is flat), an
activated region (which is also flat but at a different potential), and a
``wavefront'' region, which is described in two dimensions by a smooth but
sharp (across the wavefront) spatial function such as those used (in one
dimension) for the time waveforms in activation-based methods. This
three-region surface is then used as a constraint in an otherwise typical
Tikhonov solution. The solution can be iterated forward or backward in
time.
The size of the potential jump across the wavefront must be known.
Also any drift in the potential of the regions far from the
activation wavefront, typically caused by drift in the reference potential
used in the measurements, must be identified and eliminated. This method has
shown considerable early promise but is included here as a ``research''
approach and should be treated as such.
